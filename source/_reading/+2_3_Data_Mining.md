---
layout:     post
title:      "数据挖掘"
date:       2017-07-10
tags:
    - BigData
categories:
    - Reading
---

SYSU暑期课程的内容，讲师是澳大利亚新南威尔士大学的Wei Wang，受益匪浅，这里将一些有价值的东西在这里总结一下。

## 前言
> 教师介绍： Wei Wang : Professor @ CSE, UNSW (Ranked 42nd in CS/IS by QS Ranking 2017)
> 参考书籍：“Mining of Massive Datasets” from Stanford / “Data Mining” by Prof. Jiawei Han

1. 要有看学术前沿的论文的能力，而且要看懂看正确。
2. 对于编程语言要抓住核心，具体的语言细节不是重点，思想可能更加重要
3. 未来是会快速变的，应该要有再次学习/终生学习的能力。
4. 课堂授课相较于现在泛滥的知识来比，优点是与老师可以有交互，以及授课老师独特的观点。
5. 对高维数据的处理很重要，原因下文会提及。

## 位置（上下文)敏感的哈希
> LSH说白了就是一种给数据降维的方法，本节除了LSH以外，对于误差的消除和只有一定概率成功的随机算法的boosting思想也很重要。

1. 判断相似很重要，大数据分析很重要的操作是判断相同（似），举例来说：查询/划分。
2. 上下文关联性，语义会取决于短期上下文。长期的对计算量/复杂度要求较高。
3. 使用诸如，集合，向量，位图等数据结构来储存对应原对象的一些特性，可以使用保存相似性但是减少字段的一些手段（比如hash）来简化数据。
4. 随机算法也很重要，随机算法既不会被特定case坑到，而且只要该算法有一定的成功率就具有意义。
5. 当一个算法有常数级以上的正确率的时候，通过大量重复执行，就有一定几率可以保证其中至少有一次是正确的。（因为总错误率是指数级减少）
6. 判断相似的一些想法：相似程度，kNN（一定范围内的对象），向量的夹角。对于集合的Jaccard相似（∩/∪）和距离（1 - 相似）
7. 以文本对象为例，我们可以首先对单个单词，每若干个连续字母的组合（k-shingle ）作为一种比较的“特性”，然后可以随机选取多个正常的hash函数对这些特性处理，每个函数针对每个特性会得到一个值，我们可以选取所有值中最小一个，然后多个函数的多个值总结作为该单词一个hash值，通过合适的hash函数我们可以保留最重要的相似性和其他重要的值（当然总会有一些损失）
8. 由上，我们有理由相信在合适的哈希函数选取的情况下，通过对应hash值的比较就可以得到相似的比较结果。
9. 采取如下方式：重复b次，每次寻找r个哈希函数形成一个码，查询的时候，只要有一个码匹配即返回。
10. 上述算法会有如下的匹配率：P = 1 - (1 - x ^ r) ^ b，通过调整r/b的大小我们可以得到合适的相似度映射方式。
11. 当然除了哈希，我们也可以使用树的方法来储存。不过需要定义二分规则来决定是再左还是右。

## 预处理
> 一般在正式开始之前我们需要对数据进行一些处理，讲到了具体的操作，还有第一次引入信息熵的概念。

1. 数据清洗，对于确实项的数据，我们可以填（采取特殊值或者较大样本的均值）或者干脆不要。
2. 数据整合，对于不同表格把他们变成相同形式。
3. 数据转化，又叫规范化，大概就是将数据映射一下，有根据范围的映射，还有基于分布的映射。
4. 数据简化，减少数据字段之类的
5. 数据离散/连续化，离散化的话去噪/适合Model/减少数据字段大小，连续话得话就先平滑处理/引射到某种pdf
6. 离散化的方法，等宽（不针对数据特异），等深度，定义什么叫“好”，损失的信息最少。（求距离平方）
7. 计算最优的离散，使用递归版的动态规划，`OPT(X[1...n], B) = min i in[n] {SSE(X[1...i]) + OPT(X[i+1...n], B + 1)}`
8. 通过信息熵的角度来看，希望添加的划分对我们关心的属性尽可能相关，使得`Gain(T) = Ent(S) - E(T;S) > δ`
9. 连续化的实质是扩充值，可以直接使用概率分布算也可以使用小范围的概率分布来拟合成曲线。

## 线性代数

1. 本质还是代数，意味着对于特定物体满足基本代数运算。
2. 向量的本质，向量可以作为向量空间的基，矩阵变化本质也就是从一个向量空间映射得到一个新的坐标系的向量空间。
3. 有加法/乘法，就有逆运算减法/除法，对于变化也有对应的已知变化矩阵（映射的方式）求原空间的值。
4. 矩阵分解，“变化”的分离

## 分类

1. 分类只是预测一个物体的类别（R^a -> y），回归则是预测缺失值（R^a -> R），R^a就是对于单个物体的属性向量。
2. 得到手里的数据，首先分一下训练集和测试集。对于具体数据量大小和情况有多种分法。我们需要的是测试集的错误率尽可能小。
3. 决策树的生成算法：使得选择的当前判断尽可能的有利。
4. 判断有利的方法：相关性较大，子类概率和较大。
5. 有可能会出现过度适应的现象，可以限制高度和修剪。

## 贝叶斯分类

1. 总体思想，用贝叶斯定理，P(B|A) = P(A|B)*P(B) / P(A)
2. 我们希望知道的是知道特征A后，是特征B（比如是某一个类别）的概率，这样我们只需要比较所有P(A|B)*P(B)选最大的那个，P(B)是P(A|B)的求和，P(A)在分母上对于比较而言是相同的所以可以不用计算。
3. P(A|B)在样本过少或者A的维数过高的时候很可能会为0，此时我们采取的办法是将特征A分解，假定其独立，由概率统计相关知识我们只需要对单个概率求和就可以得到结果。
4. 即便如此如果单个的概率仍然为0，我们可以使用所有概率分子+1，分母+n的方法来平滑。
5. 简单的单个求和只是一种，也有上下文相关的，不过一般不能相关的太多，不然计算成本就太大了。
6. 使用伯努利模型变为与上下文无关的有该特性则为一的二项值。

## 判别模型

1. 该类模型的总体思想是计算已知特征下是某类别的概率，大体思路是计算一组权重，使得各个特性与权值的组合的值恰好满足某类别，由于最后我们需要计算的是一格概率，可以使用一些函数变化来映射回来。
2. 上面提交线性组合的值最好是总体最接近类别的值，可以有多种方法来估计这个接近的距离。
3. 映射回来的函数可以是logistic之类的函数。
4. 可以使用最大似然法来计算结果。

## 基于对象的学习

1. kNN，一定范围内跟着大多数对象的决定，可以通过设置范围来改变平滑程度。
2. k为一的时候，可以理解为每个对象都有一个领域，只要落到这个领域里面来就是这个类别。
3. 也可以把之前决策树的判别理解为这样的，只不过这样边界就是横平竖直的

## 聚类分析

1. 通过相似度来将一类划分到一起。
2. 判别相似程度有很多方法，比如距离，向量夹角，等等。
3. 在处理前需要先做标准化的工作，比如放缩，平均化（均值的距离/平均距离），但是特殊极值还是可能会影响一些算法。
4. 计算差异矩阵，具体的差异计算方法有很多。
5. 计算差异的公式需要满足一些基本性质（因为有些模型计算的时候需要用到） 
6. Kmeans算法是期望最大话的一种，大体的思路是先选几个中心点，使得分到这个中心点的一类点的距离和最小。可以使用期望最大化的通用迭代过程求解，但是不仅要知道初始点，初始点的位置也要选好，比如完全随机，随机一个选距离最大，以及改进版本Kmeans++的按照距离长度随机（减少极值的影响）

> 注： 由于沉迷游戏欠了接近一周才更，总之还是要补的。

## 分层方法

1. 只要能计算不同，那么最好的划分就是一个类内不同最小的。
2. 根据以上思想，可以开始划分为一块逐渐分裂，或者开始单个是单个一块逐渐合拢。

## 关联规则分析方法

1. 判断两件事的相关性，可以用于推荐系统等。
2. A-> AC = Pr(C|A) = support(AC) / support(A)
3. 那么我们的目标就是计算执行程度，思路很简单，但是如果数据过多的话，盲目遍历是不可能的。
4. 通用的广搜的话，我们一旦发现某个不存在，包含他的项都可以不用查了。
5. 如果项数本来就很多，第二层多半就GG了，这时候我们还深搜就好了。
6. 上条那个思想可以衍生出

## 偏差与方差问题

1. 大概意思就是一个小另一个就会大。
2. 自己看着办选就好。

## 综合学习

1. 这名字简直简明扼要，就是综合起来学习嗯。大致的思路就是给独立的模型加权重。
2. 一种思路是根据样本随机抽取生产新随机样本。
3. 另外可以根据模型本身的某种反应靠谱程度的数值调整权重系数。
4. 最后为了更加随机还可以随机选取输入属性，

## 总结

大概除了写这个的我不会再有其他人能看得懂了。